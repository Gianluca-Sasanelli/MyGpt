model_params:
  n_layers: 6
  n_heads: 16
  emb: 768
  dropout: 0.01

training_params:
  batch_size: 8
  context: 768
  gradient_accumulation_iter: 16
  start: <|endoftext|> 
  num_samples: 1
  max_new_tokens: 256

optimizer_params:
  max_lr: 0.001
  max_iters: 5000
  weight_decay: 0.01
  b1: 0.9 
  b2: 0.95
  grad_clip: 1.0

scheduler_params:
  decay_lr: True
  warmup_iters: 500
  lr_decay_iters: 5000 
  min_lr: 0.0001

logging_params:
  out_dir: outputs
  always_save_checkpoint: False
  eval_iters: 10
  eval_interval: 200
  log_interval: 30
  init_from: scratch
