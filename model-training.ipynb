{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BmV8hiMfykEp"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4rvL5EOrypt4",
        "outputId": "d96ac312-fe36-4bc5-ee45-ec5ca73c6b8e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "import math, os, torch, inspect, time, tiktoken\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn as nn\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader,random_split\n",
        "import torch.nn.functional as F\n",
        "torch.manual_seed(42)\n",
        "torch.set_float32_matmul_precision(\"medium\")\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1eod2SAd1PqE"
      },
      "source": [
        "\n",
        "# Embeddings, feedforward, Heads"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "9hEU5rLk1ULH"
      },
      "outputs": [],
      "source": [
        "class FeedF_norm(nn.Module):\n",
        "  def __init__(self, context, emb,  dropout):\n",
        "    super().__init__()\n",
        "    self.lin1 = nn.Linear(emb, 4 * emb, bias = False) \n",
        "    self.act1 = nn.GELU()\n",
        "    self.drop= nn.Dropout(dropout)\n",
        "    self.output_projection = nn.Linear(4 * emb, emb, bias = False)\n",
        "\n",
        "  def forward(self, x: torch.Tensor):\n",
        "    x = self.act1(self.lin1(x))\n",
        "    x = self.drop(self.output_projection(x))\n",
        "    return x\n",
        "    \n",
        "class Attention_block(nn.Module):\n",
        "  \n",
        "  def __init__(self,  n_heads: int, emb: int, context: int, dropout: float):\n",
        "    super().__init__()\n",
        "    assert emb % n_heads == 0, \"embedding dimensionality must be a multiple of n_heads\"\n",
        "    self.emb = emb\n",
        "    self.nh = n_heads\n",
        "    self.dropout_p = dropout\n",
        "    self.projection = nn.Linear(emb, emb*3)\n",
        "    self.output_projection = nn.Linear(emb, emb)\n",
        "    self.output_dropout = nn.Dropout(dropout)\n",
        "    self.flash = hasattr(nn.functional, \"scaled_dot_product_attention\")\n",
        "    if not self.flash:\n",
        "      print(\"flesh is not working\")\n",
        "      self.register_buffer(\"causal_mask\", torch.tril(torch.ones(self.context, self.contex)\n",
        "                                                     ).view(1,1, self.context, self.context))\n",
        "      self.att_drop = nn.Dropout(dropout)\n",
        "      \n",
        "  def get_attention(self, query: torch.Tensor, keys: torch.Tensor, values: torch.tensor):\n",
        "    if self.flash:\n",
        "      out = torch.nn.functional.scaled_dot_product_attention(query, keys, values, attn_mask=None, dropout_p=self.dropout_p if self.training else 0,\n",
        "                                                             is_causal=True)\n",
        "    else: \n",
        "      scale = math.sqrt(self.emb)\n",
        "      att_scores = (query @ keys.transpose(-2,-1))/scale \n",
        "      B, NH, C, E = query.shape\n",
        "      att_scores.masked_fill_(self.causal_mask[:,:,:C, :C] == 0, float('-inf'))\n",
        "      att_scores = att_scores.softmax(y, dim = -1)\n",
        "      att_scores = self.att_drop(att_scores)\n",
        "      out = att_scores @ values\n",
        "    return out\n",
        "\n",
        "\n",
        "  def forward(self, x: torch.Tensor):\n",
        "    B, C, E = x.shape\n",
        "    q, k, v = self.projection(x).split(self.emb, dim = 2) # batch, context, emb\n",
        "    q = q.view(B, C, self.nh, E // self.nh).transpose(1,2) # b, nh, c, emb\n",
        "    k = k.view(B, C, self.nh, E // self.nh).transpose(1,2)\n",
        "    v = v.view(B, C, self.nh, E // self.nh).transpose(1,2)\n",
        "    x = self.get_attention(q, k, v) # b, nh, c, c\n",
        "    x = x.transpose(1,2).contiguous().view(B, C, E)\n",
        "    x = self.output_dropout(self.output_projection(x))\n",
        "    return x\n",
        "  \n",
        "class TransformerBlock(nn.Module):\n",
        "  def __init__(self, context, emb, n_heads,dropout):\n",
        "    super().__init__()\n",
        "    self.ln1 = nn.LayerNorm(emb, bias = False)\n",
        "    self.MultiHead = Attention_block(n_heads, emb, context, dropout)\n",
        "    self.ln2 = nn.LayerNorm(emb, bias = False)\n",
        "    self.FeedForward = FeedF_norm(context, emb,dropout)\n",
        "    \n",
        "  def forward(self, x: torch.Tensor):\n",
        "    x = x + self.MultiHead(self.ln1(x)) #batch, context, d_model \n",
        "    x = x + self.FeedForward(self.ln2(x)) #(B,C,D)\n",
        "    return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XhYIm-W31Vtd"
      },
      "source": [
        "# Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "5zxJvEYQ2alt"
      },
      "outputs": [],
      "source": [
        "class GPT1(nn.Module):\n",
        "  def __init__(self,context, emb, vocab_size, n_layers, n_heads,  dropout):\n",
        "    super().__init__()\n",
        "    self.drop = nn.Dropout(dropout)\n",
        "    self.emb = nn.Embedding(vocab_size, emb)\n",
        "    self.pos = nn.Embedding(context, emb)\n",
        "    self.context = context\n",
        "    self.TransformerBlocks = nn.ModuleList(\n",
        "        [TransformerBlock(context, emb, n_heads,dropout)\n",
        "          for _ in range(n_layers)])\n",
        "    self.generator = nn.Linear(emb, vocab_size, bias = False)\n",
        "    self.emb.weight =  self.generator.weight\n",
        "    self.apply(self._init_weights)\n",
        "    #normalization for residuals\n",
        "    for pn, p in self.named_parameters():\n",
        "            if pn.endswith('output_projection.weight'):\n",
        "                torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * n_layers))\n",
        "                \n",
        "  def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)   \n",
        "             \n",
        "  def get_num_params(self, non_embedding=True):\n",
        "\n",
        "    n_params = sum(p.numel() for p in self.parameters())\n",
        "    if non_embedding:\n",
        "        n_params -= self.pos.weight.numel()\n",
        "    return n_params          \n",
        "            \n",
        "  def configure_optimizers(self, weight_decay, learning_rate, betas, device_type):\n",
        "    # Getting parameters with requires_grad = True\n",
        "      param_dict = {pn: p for pn, p in self.named_parameters()}\n",
        "      param_dict = {pn: p for pn, p in param_dict.items() if p.requires_grad}\n",
        "      # create optim groups. Any parameters that is 2D will be weight decayed, otherwise no.\n",
        "      # i.e. all weight tensors in matmuls + embeddings decay, all biases and layernorms don't.\n",
        "      decay_params = [p for n, p in param_dict.items() if p.dim() >= 2]\n",
        "      nodecay_params = [p for n, p in param_dict.items() if p.dim() < 2]\n",
        "      optim_groups = [\n",
        "          {'params': decay_params, 'weight_decay': weight_decay},\n",
        "          {'params': nodecay_params, 'weight_decay': 0.0}\n",
        "      ]\n",
        "      num_decay_params = sum(p.numel() for p in decay_params)\n",
        "      num_nodecay_params = sum(p.numel() for p in nodecay_params)\n",
        "      print(f\"num decayed parameter tensors: {len(decay_params)}, with {num_decay_params:,} parameters\")\n",
        "      print(f\"num non-decayed parameter tensors: {len(nodecay_params)}, with {num_nodecay_params:,} parameters\")\n",
        "      optimizer = torch.optim.AdamW(optim_groups, lr=learning_rate, betas=betas)\n",
        "      return optimizer    \n",
        "\n",
        "  def forward(self, x: torch.Tensor, targets = None):\n",
        "    B, C = x.shape\n",
        "    embs = self.emb(x) #batch, seq, emb\n",
        "    pos = self.pos(torch.tensor([i for i in range(C)],dtype = torch.int64, device=device ))\n",
        "    x = self.drop(embs + pos)\n",
        "    for Transf in self.TransformerBlocks:\n",
        "      x = Transf(x)\n",
        "    if targets is not None:\n",
        "        # if we are given some desired targets also calculate the loss\n",
        "        logits = self.generator(x)\n",
        "        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
        "    else:\n",
        "        logits = self.generator(x[:, [-1], :]) \n",
        "        loss = None    \n",
        "    return logits, loss\n",
        "  \n",
        "  #generation from previous tokens\n",
        "  @torch.no_grad()\n",
        "  def generate(self, idx, max_tokens):\n",
        "    B = idx.shape[0]\n",
        "    for _ in range(max_tokens):\n",
        "      new_idx = idx[:, -self.context:]\n",
        "      logits, _ = self(new_idx)\n",
        "      probs = logits.softmax(dim = -1)[:,0,:]\n",
        "      next_toks = torch.multinomial(probs, num_samples =1) \n",
        "      idx = torch.cat((idx, next_toks), dim = 1)\n",
        "    for i in range(B):\n",
        "      print(tokenizer.decode(list(idx[i,:])))\n",
        "  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "params = {\n",
        "    'context': 256, #context that the model can see\n",
        "    'emb': 256,\n",
        "    'vocab_size': 50304, # total number of tokens (gpt2)\n",
        "    'n_layers': 4, #number of transformer layer\n",
        "    'n_heads': 8, #number of head per layer\n",
        "    'dropout': 0.2\n",
        "}\n",
        "model = GPT1(**params ) #Initialization of the model with the given parameters\n",
        "model = model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "data_dir = 'data'\n",
        "def get_batch(split):\n",
        "    if split == 'train':\n",
        "        data = np.memmap(os.path.join(data_dir, 'train.bin'), dtype=np.uint16, mode='r')\n",
        "    else:\n",
        "        data = np.memmap(os.path.join(data_dir, 'val.bin'), dtype=np.uint16, mode='r')\n",
        "    ix = torch.randint(len(data) - context, (batch,))\n",
        "    x = torch.stack([torch.from_numpy((data[i:i+context]).astype(np.int64)) for i in ix])\n",
        "    y = torch.stack([torch.from_numpy((data[i+1:i+1+context]).astype(np.int64)) for i in ix])\n",
        "    \n",
        "    if device == 'cuda':\n",
        "        # pin arrays x,y, which allows us to move them to GPU asynchronously (non_blocking=True)\n",
        "        x, y = x.pin_memory().to(device, non_blocking=True), y.pin_memory().to(device, non_blocking=True)\n",
        "    else:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "    return x, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "num decayed parameter tensors: 18, with 7,241,728 parameters\n",
            "num non-decayed parameter tensors: 16, with 3,072 parameters\n"
          ]
        }
      ],
      "source": [
        "context = 256\n",
        "batch = 16\n",
        "\n",
        "\n",
        "learning_rate = 6e-4\n",
        "weight_decay = 1e-1\n",
        "beta1 = 0.9\n",
        "beta2 = 0.95\n",
        "grad_clip = 1.0 #\n",
        "min_lr = 6e-5\n",
        "optimizer = model.configure_optimizers(weight_decay, learning_rate, (beta1, beta2), \"cuda\")\n",
        "warmup_iters = 500\n",
        "lr_decay_iters = 10000\n",
        "\n",
        "max_steps = 10000\n",
        "text = \"Hello, I am a language model\"\n",
        "tokenizer= tiktoken.get_encoding(\"gpt2\")\n",
        "idx = torch.tensor(tokenizer.encode(text), device = device).unsqueeze(0).repeat(5, 1)\n",
        "tokens_processed = batch * context\n",
        "def cosine_scheduler(it):\n",
        "    # 1) linear warmup for warmup_iters steps\n",
        "    if it < warmup_iters:\n",
        "        return learning_rate * (it + 1) / warmup_iters\n",
        "    # 2) if it > lr_decay_iters, return min learning rate\n",
        "    if it > lr_decay_iters:\n",
        "        return min_lr\n",
        "    # 3) in between, use cosine decay down to min learning rate\n",
        "    decay_ratio = (it - warmup_iters) / (lr_decay_iters - warmup_iters)\n",
        "    assert 0 <= decay_ratio <= 1\n",
        "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio)) # coeff ranges 0..1\n",
        "    return min_lr + coeff * (learning_rate - min_lr)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "ename": "RuntimeError",
          "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[7], line 12\u001b[0m\n\u001b[0;32m     10\u001b[0m     param_group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m lr\n\u001b[0;32m     11\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 12\u001b[0m logits, loss \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     14\u001b[0m norm \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;241m1.0\u001b[39m)\n",
            "File \u001b[1;32mc:\\Users\\Gianl\\anaconda3\\envs\\CNN\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\Gianl\\anaconda3\\envs\\CNN\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "Cell \u001b[1;32mIn[3], line 57\u001b[0m, in \u001b[0;36mGPT1.forward\u001b[1;34m(self, x, targets)\u001b[0m\n\u001b[0;32m     55\u001b[0m embs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39memb(x) \u001b[38;5;66;03m#batch, seq, emb\u001b[39;00m\n\u001b[0;32m     56\u001b[0m pos \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos(torch\u001b[38;5;241m.\u001b[39mtensor([i \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(C)],dtype \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mint64, device\u001b[38;5;241m=\u001b[39mdevice ))\n\u001b[1;32m---> 57\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop(\u001b[43membs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mpos\u001b[49m)\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m Transf \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mTransformerBlocks:\n\u001b[0;32m     59\u001b[0m   x \u001b[38;5;241m=\u001b[39m Transf(x)\n",
            "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
          ]
        }
      ],
      "source": [
        "train_losses = []\n",
        "val_losses = []\n",
        "\n",
        "for iteration in range(max_steps):\n",
        "    if iteration % 200 != 0 or iteration == 0:\n",
        "        t0 = time.time()\n",
        "        data, labels = get_batch(\"train\")\n",
        "        lr = cosine_scheduler(iteration)\n",
        "        for param_group in optimizer.param_groups:\n",
        "            param_group[\"lr\"] = lr\n",
        "        optimizer.zero_grad()\n",
        "        logits, loss = model(data, labels)\n",
        "        loss.backward()\n",
        "        norm = torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "        torch.cuda.synchronize()\n",
        "        t1 = time.time()\n",
        "        dt = t1 -t0\n",
        "        print(f\"step {iteration} | lr : {lr:6f}| loss: { loss.item():.6f} | norm: {norm:.4f} | dt: { dt*1000}ms | ms / tokens_processed: { (dt / tokens_processed):.5f}\")\n",
        "        if iteration % 20 == 0:\n",
        "            train_losses.append(loss.item())\n",
        "    else:\n",
        "        for i in range(10):\n",
        "            data, labels = get_batch(\"val\")\n",
        "            logits, loss = model(data, labels)\n",
        "            print(f\"val| loss: { loss.item():.6f}\")\n",
        "            val_losses.append(loss.item())\n",
        "        model.generate(idx, max_tokens = 20)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'plt' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241m.\u001b[39mscatter(np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(train_losses), \u001b[38;5;241m1\u001b[39m), train_losses)\n",
            "\u001b[1;31mNameError\u001b[0m: name 'plt' is not defined"
          ]
        }
      ],
      "source": [
        "plt.scatter(np.arange(0, len(train_losses), 1), train_losses)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
